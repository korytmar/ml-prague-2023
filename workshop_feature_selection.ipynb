{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Install package not available in Colab by default\n",
        "!pip install boruta"
      ],
      "metadata": {
        "id": "DqQFReYUBkYy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data load and processing\n",
        "from google.colab import files\n",
        "import io\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Models-related\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression, OrthogonalMatchingPursuit\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "from sklearn.preprocessing import normalize, StandardScaler\n",
        "from sklearn import metrics\n",
        "\n",
        "from boruta import BorutaPy\n",
        "\n",
        "# Visualisations\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Numbers display set-up\n",
        "pd.set_option(\"display.max_columns\", None)\n",
        "pd.set_option(\"display.max_rows\", None)\n",
        "pd.set_option(\"display.float_format\", lambda x: \"%.7f\" % x)"
      ],
      "metadata": {
        "id": "IfewIBAUHUeF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1) Data Preparation"
      ],
      "metadata": {
        "id": "TjuDuMPbSdQT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jRxN8sw_GcQ4"
      },
      "outputs": [],
      "source": [
        "uploaded = files.upload()\n",
        "df = pd.read_csv(io.BytesIO(uploaded[\"hr-data.csv\"]))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe(include=\"all\")"
      ],
      "metadata": {
        "id": "lndcap-y_CNa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "mYUcGg3x9m_7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# replace NaNs with string so we create corresponding category later on\n",
        "df.replace(np.nan, \"nan\", regex=True, inplace=True)\n",
        "# parse employee's current rating\n",
        "df[\"current_rating\"] = df.current_rating.str[0].astype(int)\n",
        "# drop features with explanatory power\n",
        "df.drop(columns=[\"legal_full_name\", \"employee_id\", \"manager_id\"], inplace=True)"
      ],
      "metadata": {
        "id": "1OK1EFpzKTWB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create categorical variables because our linear regression based methods accept only numerical values as an input\n",
        "worker_data = pd.get_dummies(df, prefix_sep=\" -> \")\n",
        "print(f\"Our dataset has {worker_data.shape[0]} rows and {worker_data.shape[1]} columns.\")"
      ],
      "metadata": {
        "id": "umaEK-wZHpEe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "worker_data.head()"
      ],
      "metadata": {
        "id": "DioMyVEO-_NC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of one hot encoding\n",
        "df.country.head()"
      ],
      "metadata": {
        "id": "GAMFTkJ-yeb-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "worker_data[[c for c in worker_data.columns if c.startswith(\"country\")]].head()"
      ],
      "metadata": {
        "id": "0ksuk_d_y6Ux"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2) Linear Regression"
      ],
      "metadata": {
        "id": "UA97EyJYNmcd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are a few assumptions associated with a linear regression model:\n",
        "\n",
        "* Linearity: The relationship between X and Y is linear.\n",
        "* Homoscedasticity: The variance of residual is the same for any value of X.\n",
        "* No multicollinearity: There is no high correlations among two or more independent variables.\n",
        "* Independence: Observations are independent of each other.\n",
        "* Normality: For any fixed value of X, Y is normally distributed."
      ],
      "metadata": {
        "id": "gUExB_0FwI7y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# avoid adding additional multicollinearity by dropping one category per each feature\n",
        "worker_data = pd.get_dummies(df, prefix_sep=\" -> \", drop_first=True)\n",
        "print(f\"Our dataset has {worker_data.shape[0]} rows and {worker_data.shape[1]} columns.\")"
      ],
      "metadata": {
        "id": "UsW7R-OhN5YJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_data = worker_data.drop(\"total_compensation\", axis=1)\n",
        "y_data = worker_data[\"total_compensation\"]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_data, y_data, test_size=0.20, random_state=12\n",
        ")\n",
        "column_names = X_train.columns"
      ],
      "metadata": {
        "id": "Sj40JNugOSqf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Standard scaler uses this formula: z = (x - u) / s\n",
        "# which results in 0 mean 1 std for all features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "EsTtuiYjNvJo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(6, 5))\n",
        "ax1.set_title(\"Before feature scaling\")\n",
        "sns.kdeplot(X_train[\"age\"], ax=ax1)\n",
        "sns.kdeplot(X_train[\"length_of_service\"], ax=ax1)\n",
        "ax2.set_title(\"After feature scaling\")\n",
        "sns.kdeplot(X_train_scaled[:, 7], ax=ax2)\n",
        "sns.kdeplot(X_train_scaled[:, 6], ax=ax2)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "KHsHzoX2BXzE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a linear regression model\n",
        "model = LinearRegression()\n",
        "# Train the model\n",
        "model.fit(X_train_scaled, y_train)\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "residuals = y_test - y_pred\n",
        "# Evaluate the model\n",
        "rmse = metrics.mean_squared_error(y_test, y_pred, squared=False)\n",
        "r2 = metrics.r2_score(y_test, y_pred)\n",
        "mape = metrics.mean_absolute_percentage_error(y_test, y_pred)\n",
        "\n",
        "print(\"Mean Absolute Percentage Error (MAPE):\", round(mape * 100, 2), \"%\")\n",
        "print(\"Root Mean Squared Error:\", rmse)\n",
        "print(\"R-squared:\", r2)"
      ],
      "metadata": {
        "id": "TEQ0B8iOLeAE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We broke multicollinearity assumption by using all features\n",
        "f = plt.figure(figsize=(19, 15))\n",
        "plt.matshow(X_train.corr(), fignum=f.number)\n",
        "cb = plt.colorbar()\n",
        "cb.ax.tick_params(labelsize=14)\n",
        "plt.title('Correlation Matrix', fontsize=16);\n",
        "# NOTE: White line represents columns which contain only zeros"
      ],
      "metadata": {
        "id": "9tHE8n-76Ou2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3) Orthogonal Matching Pursuit"
      ],
      "metadata": {
        "id": "iZEb4U5RY-XE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# OMP model can handle multicollinearity\n",
        "worker_data = pd.get_dummies(df, prefix_sep=\" -> \", drop_first=False)\n",
        "print(f\"Our dataset has {worker_data.shape[0]} rows and {worker_data.shape[1]} columns.\")"
      ],
      "metadata": {
        "id": "CATELUHFZrel"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_data = worker_data.drop(\"total_compensation\", axis=1)\n",
        "y_data = worker_data[\"total_compensation\"]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_data, y_data, test_size=0.20, random_state=12\n",
        ")"
      ],
      "metadata": {
        "id": "i_sbtBTIZxDB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "Eey4lJ-IZ0po"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a OMP model\n",
        "n_coefs = 20\n",
        "model = OrthogonalMatchingPursuit(n_nonzero_coefs=n_coefs)\n",
        "# Train the model\n",
        "model.fit(X_train_scaled, y_train)\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "residuals = y_test - y_pred\n",
        "# Evaluate the model\n",
        "rmse = metrics.mean_squared_error(y_test, y_pred, squared=False)\n",
        "r2 = metrics.r2_score(y_test, y_pred)\n",
        "mape = metrics.mean_absolute_percentage_error(y_test, y_pred)\n",
        "\n",
        "print(\"Mean Absolute Percentage Error (MAPE):\", round(mape * 100, 2), \"%\")\n",
        "print(\"Root Mean Squared Error:\", rmse)\n",
        "print(\"R-squared:\", r2)"
      ],
      "metadata": {
        "id": "O3fa2AfuFuZK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualise the features that have been selected and their respective scores\n",
        "feature_scores = pd.Series(model.coef_, index=X_data.columns).sort_values(ascending=False, key=lambda x: abs(x))\n",
        "selected_features = feature_scores[:n_coefs]\n",
        "\n",
        "f, ax = plt.subplots(figsize=(10, 5))\n",
        "shades = 31\n",
        "palette = sns.color_palette(\"coolwarm\", shades)\n",
        "minmax = np.max([-np.min(selected_features), np.max(selected_features)])\n",
        "bins = np.linspace(-minmax, minmax, num=shades)\n",
        "palette_indices = np.digitize(selected_features, bins) - 1\n",
        "colors = [palette[idx] for idx in palette_indices]\n",
        "\n",
        "ax = sns.barplot(x=selected_features, y=selected_features.index, palette=colors, dodge=False)\n",
        "ax.set_yticklabels(selected_features.index)\n",
        "ax.set_xlabel(\"Feature impact on prediction\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ATTn1VztZ8n-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercises"
      ],
      "metadata": {
        "id": "2xNwGUKhHbJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### A. Implement Orthogonal Matching Pursuit: Write code to implement the Orthogonal Matching Pursuit algorithm from scratch. Given a dictionary matrix, a target signal, and a desired sparsity level, apply OMP to approximate the target signal using a sparse representation.\n"
      ],
      "metadata": {
        "id": "rEhS20ZNHfzs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rng = np.random.default_rng(12345)\n",
        "n_samples = 10\n",
        "age = rng.normal(45, 12, size=n_samples)\n",
        "length_of_service = rng.normal(2, 1, size=n_samples)\n",
        "current_rating = rng.integers(1, 5, endpoint=True, size=n_samples)\n",
        "is_manager = rng.binomial(1, 0.2, size=n_samples)\n",
        "\n",
        "# total compensation rounded to nearest thousand dollars\n",
        "total_compensation = np.array([194, 120, 120, 115, 131, 143, 128, 165, 129, 132])"
      ],
      "metadata": {
        "id": "tR2uYWkr6_-1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_data = pd.DataFrame(np.column_stack([age, length_of_service, current_rating, is_manager]), columns=[\"age\", \"length_of_service\", \"current_rating\", \"is_manager\"])"
      ],
      "metadata": {
        "id": "qVZscKa8viaJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_data"
      ],
      "metadata": {
        "id": "0LV5LXxl-Wwz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_compensation"
      ],
      "metadata": {
        "id": "Ou90PW38QzvH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(input_data)"
      ],
      "metadata": {
        "id": "EyOui0CC_sR0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TASK: Estimate coefficients for each vector in dataset (dictionary) such that target (signal) can be approximated by their linear combination. Moreover, try to minimize number of used features (atoms)."
      ],
      "metadata": {
        "id": "vFCpeE5EBJdg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def omp(X, y, sparsity):\n",
        "  ...\n",
        "  # TODO\n"
      ],
      "metadata": {
        "id": "fM-mJHLjQ_3e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We have 4 features, let's examine their contribution for our target variable.\n",
        "# The contribution is defined as scalar product of each feature with the target.\n",
        "\n",
        "# e.g. contribution of 1st feature (\"age\") in the target\n",
        "# element-wise multiplication and then summation\n",
        "age_mul = X_train[:, 0] * total_compensation\n",
        "age_contr = np.sum(age_mul)\n",
        "print(\"Contribution of 'age' feature is:\", age_contr)"
      ],
      "metadata": {
        "id": "quHuGb0OBFGI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We will continue with other features and their corresponding effect in the target\n",
        "len_of_service_contr = np.sum(X_train[:, 1] * total_compensation)\n",
        "print(\"Contribution of 'length_of_service' feature is:\", len_of_service_contr)\n",
        "\n",
        "current_rating_contr = np.sum(X_train[:, 2] * total_compensation)\n",
        "print(\"Contribution of 'current_rating' feature is:\", current_rating_contr)\n",
        "\n",
        "is_manager_contr = np.sum(X_train[:, 3] * total_compensation)\n",
        "print(\"Contribution of 'is_manager' feature is:\", is_manager_contr)"
      ],
      "metadata": {
        "id": "g83_x52UDK9M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's look at the absolute maximum contribution\n",
        "contributions = [age_contr, len_of_service_contr, current_rating_contr, is_manager_contr]\n",
        "abs_contributions = np.abs(contributions)"
      ],
      "metadata": {
        "id": "UmN576g1D0ga"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's get the index (feature name) which belongs to the maximum value\n",
        "idx = np.argmax(abs_contributions)\n",
        "print(f\"'{input_data.columns[idx]}' contributes the most to the target variable.\")"
      ],
      "metadata": {
        "id": "EA4Ko5FMEkWU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The code above can be rewritten using numpy matrix operations\n",
        "contrs = X_train.T @ total_compensation\n",
        "abs_contrs = np.abs(contrs)\n",
        "idx = np.argmax(abs_contrs)\n",
        "print(f\"'{input_data.columns[idx]}' contributes the most to the target variable.\")"
      ],
      "metadata": {
        "id": "aH1WfFbUFMuu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Our new dataset (basis) consists of one feature (\"length_of_service\"). Now,\n",
        "# we can estimate the coefficient for this feature. We will leverage numpy implementation\n",
        "# of calculating pseudo inverse of matrix (A^+ = (A^T * A)^-1 * A^T) when finding\n",
        "# the matrix that solves the least-squares problem A * x = b. This equation can\n",
        "# be rewritten as x = A^-1 * b.\n",
        "inverse_matrix = np.linalg.pinv(X_train[:, [idx]])\n",
        "coefficient = inverse_matrix @ total_compensation"
      ],
      "metadata": {
        "id": "B-IaK4_2JwBw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Estimated coefficient for feature '{input_data.columns[idx]}' is: {coefficient}\")"
      ],
      "metadata": {
        "id": "cDLrwoM1MMTv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's see how well we can estimate our target variable just by using `coefficients`.\n",
        "# We will have to change the shape of coefficients such that it has the same number of rows\n",
        "# as the input matrix has columns.\n",
        "out_coefficients = np.zeros(shape=X_train.shape[1])\n",
        "out_coefficients[idx] = coefficient\n",
        "predictions = X_train @ out_coefficients"
      ],
      "metadata": {
        "id": "3BMXYgJCMbzd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Predicitons for total compensation:\", predictions)"
      ],
      "metadata": {
        "id": "e72TlHwUOhBI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We can easily see that our predictions are significantly off from our target variable.\n",
        "# So, we subtract current predictions from total compensation and we will use it in next\n",
        "# iteration as the target variable. We call this variable residuals.\n",
        "residuals = total_compensation - predictions\n",
        "print(\"Residuals after 1st iteration:\", residuals)"
      ],
      "metadata": {
        "id": "LJnWOHPMPCx9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's create a block of code of what we have just seen implemented\n",
        "\n",
        "# Standardize dataset\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(input_data)\n",
        "\n",
        "# Define initial state of variables\n",
        "residuals = total_compensation\n",
        "indices = []\n",
        "coefficients = np.zeros(shape=X_train.shape[1])"
      ],
      "metadata": {
        "id": "XD7EFRMyTGB6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_rmse(residuals):\n",
        "    return np.sqrt(np.sum(residuals ** 2) / len(residuals))"
      ],
      "metadata": {
        "id": "IwBj12XyWMYW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1st iteration\n",
        "abs_contributions = np.abs(X_train.T @ residuals)\n",
        "idx = np.argmax(abs_contributions)\n",
        "indices.append(idx)\n",
        "print(f\"Selected index is: {idx}, i.e. '{input_data.columns[idx]}' feature.\")\n",
        "\n",
        "inverse_matrix = np.linalg.pinv(X_train[:, indices])\n",
        "current_coefficients = inverse_matrix @ total_compensation\n",
        "coefficients[indices] = current_coefficients\n",
        "predictions = X_train @ coefficients\n",
        "residuals = total_compensation - predictions\n",
        "print(\"Residuals remaining to explain:\", residuals)\n",
        "print(\"RMSE:\", calculate_rmse(residuals))"
      ],
      "metadata": {
        "id": "RsMdQ-WRToHa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2nd iteration\n",
        "abs_contributions = np.abs(X_train.T @ residuals)\n",
        "idx = np.argmax(abs_contributions)\n",
        "indices.append(idx)\n",
        "print(f\"Selected index is: {idx}, i.e. '{input_data.columns[idx]}' feature.\")\n",
        "\n",
        "inverse_matrix = np.linalg.pinv(X_train[:, indices])\n",
        "current_coefficients = inverse_matrix @ total_compensation\n",
        "coefficients[indices] = current_coefficients\n",
        "predictions = X_train @ coefficients\n",
        "residuals = total_compensation - predictions\n",
        "print(\"Residuals remaining to explain:\", residuals)\n",
        "print(\"RMSE:\", calculate_rmse(residuals))"
      ],
      "metadata": {
        "id": "E2BM7nkuTn4D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3rd iteration\n",
        "abs_contributions = np.abs(X_train.T @ residuals)\n",
        "idx = np.argmax(abs_contributions)\n",
        "indices.append(idx)\n",
        "print(f\"Selected index is: {idx}, i.e. '{input_data.columns[idx]}' feature.\")\n",
        "\n",
        "inverse_matrix = np.linalg.pinv(X_train[:, indices])\n",
        "current_coefficients = inverse_matrix @ total_compensation\n",
        "coefficients[indices] = current_coefficients\n",
        "predictions = X_train @ coefficients\n",
        "residuals = total_compensation - predictions\n",
        "print(\"Residuals remaining to explain:\", residuals)\n",
        "print(\"RMSE:\", calculate_rmse(residuals))"
      ],
      "metadata": {
        "id": "Y69FCMRZYXzN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4th iteration\n",
        "abs_contributions = np.abs(X_train.T @ residuals)\n",
        "idx = np.argmax(abs_contributions)\n",
        "indices.append(idx)\n",
        "print(f\"Selected index is: {idx}, i.e. '{input_data.columns[idx]}' feature.\")\n",
        "\n",
        "inverse_matrix = np.linalg.pinv(X_train[:, indices])\n",
        "current_coefficients = inverse_matrix @ total_compensation\n",
        "coefficients[indices] = current_coefficients\n",
        "predictions = X_train @ coefficients\n",
        "residuals = total_compensation - predictions\n",
        "print(\"Residuals remaining to explain:\", residuals)\n",
        "print(\"RMSE:\", calculate_rmse(residuals))"
      ],
      "metadata": {
        "id": "QOxm6Ac-Yydh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We end the algorithm because we used all features. The residuals converge but \n",
        "# let's take a look at the error on the dataset.\n",
        "rmse = calculate_rmse(residuals)\n",
        "print(\"RMSE:\", rmse)"
      ],
      "metadata": {
        "id": "RVc4PSWQY8kb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The error above looks like we did not learn anything about total compensation from data.\n",
        "# The missing term is intercept. Intercept is defined as `mean(y) - mean(x1)* coef1 - mean(x2) * coef2 - ...`\n",
        "# where `y` is target `x1` is the 1st feature and `coef1` is the 1st coefficient\n",
        "# corrresponding to the 1st feature. All terms except for `mean(y)` vanishes given that\n",
        "# we standardized our input those features will have mean(x) = 0.\n",
        "intercept = np.mean(total_compensation)\n",
        "residuals = total_compensation - predictions - intercept"
      ],
      "metadata": {
        "id": "jk365PD4sHMv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's delve into the error on the dataset once again.\n",
        "rmse = np.sqrt(np.sum(residuals ** 2) / len(residuals))\n",
        "print(\"RMSE:\", rmse)"
      ],
      "metadata": {
        "id": "E_mpOpL2tUrg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We can finally rewrite the steps into a function.\n",
        "def omp(X, y, sparsity):\n",
        "    num_atoms = X.shape[1]\n",
        "    residuals = y\n",
        "    selected_features = []\n",
        "    coefficients = np.zeros(num_atoms)\n",
        "\n",
        "    for _ in range(sparsity):\n",
        "        abs_contributions = np.abs(X.T @ residuals)\n",
        "        best_feature = np.argmax(abs_contributions)\n",
        "        selected_features.append(best_feature)\n",
        "\n",
        "        X_ = X[:, selected_features]\n",
        "        curr_coefficients = np.linalg.pinv(X_) @ y\n",
        "        coefficients[selected_features] = curr_coefficients\n",
        "\n",
        "        prediction = X @ coefficients\n",
        "        residuals = y - prediction\n",
        "\n",
        "    intercept = y.mean()\n",
        "    return coefficients, intercept"
      ],
      "metadata": {
        "id": "7fHuBU44thng"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Solution\n",
        "# Implementation of OMP algorithm\n",
        "def omp(dictionary, signal, sparsity):\n",
        "    num_atoms = dictionary.shape[1]\n",
        "    residual = signal\n",
        "    support = []\n",
        "    coefficients = np.zeros(num_atoms)\n",
        "\n",
        "    for _ in range(sparsity):\n",
        "        correlations = np.abs(np.dot(dictionary.T, residual))\n",
        "        best_atom = np.argmax(correlations)\n",
        "        support.append(best_atom)\n",
        "\n",
        "        selected_atoms = dictionary[:, support]\n",
        "        projection = np.dot(np.linalg.pinv(selected_atoms), signal)\n",
        "        coefficients[support] = projection\n",
        "\n",
        "        prediction = np.dot(dictionary, coefficients)\n",
        "        residual = signal - prediction\n",
        "\n",
        "    intercept = signal.mean()\n",
        "    return coefficients, intercept"
      ],
      "metadata": {
        "cellView": "form",
        "id": "69MPqRRLHZOi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### B. Sparse Signal Recovery: Use OMP to recover the original sparse signal from the measurements. Compare the obtained approximation with the ground truth signal."
      ],
      "metadata": {
        "id": "D_Q5uZu9HqwS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "worker_data = pd.get_dummies(df, prefix_sep=\" -> \", drop_first=False)\n",
        "print(f\"Our dataset has {worker_data.shape[0]} rows and {worker_data.shape[1]} columns.\")\n",
        "\n",
        "X_data = worker_data.drop(\"total_compensation\", axis=1)\n",
        "y_data = worker_data[\"total_compensation\"]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_data, y_data, test_size=0.20, random_state=12\n",
        ")\n",
        "column_names = X_train.columns\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "sparsity = 11\n",
        "coefficients, intercept = omp(X_train, y_train, sparsity)"
      ],
      "metadata": {
        "id": "9vWFK98OGrUz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate predictions using estimated coefficients and testing dataset\n",
        "y_pred = # TODO\n",
        "# Evaluate the model\n",
        "rmse = metrics.mean_squared_error(y_test, y_pred, squared=False)\n",
        "r2 = metrics.r2_score(y_test, y_pred)\n",
        "\n",
        "print(\"Root Mean Squared Error:\", rmse)\n",
        "print(\"R-squared:\", r2)"
      ],
      "metadata": {
        "id": "Y2GsC34CPUlK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Solution\n",
        "# Calculate predictions using estimated coefficients and testing dataset\n",
        "y_pred = X_test @ coefficients + intercept\n",
        "# Evaluate the model\n",
        "rmse = metrics.mean_squared_error(y_test, y_pred, squared=False)\n",
        "r2 = metrics.r2_score(y_test, y_pred)\n",
        "\n",
        "print(\"Root Mean Squared Error:\", rmse)\n",
        "print(\"R-squared:\", r2)"
      ],
      "metadata": {
        "id": "aaxJxesG0BMg",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### C. Visualize Sparse Signal: Use coefficients estimated by OMP to visualize their impact using (e.g.: seaborn, matplotlib). Think about the interpretation of the visualization, especially \"age\", \"job_level -> M8\" and \"generation -> Baby Boomers (1946 - 1964)\"."
      ],
      "metadata": {
        "id": "akfeHzMBSwAx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualise the features that have been selected and their respective scores\n",
        "feature_scores = pd.Series(coefficients, index=column_names).sort_values(ascending=False, key=lambda x: abs(x))\n",
        "selected_features = feature_scores[:sparsity]\n",
        "\n",
        "f, ax = plt.subplots(figsize=(10, 5))\n",
        "shades = 31\n",
        "palette = sns.color_palette(\"coolwarm\", shades)\n",
        "minmax = np.max([-np.min(selected_features), np.max(selected_features)])\n",
        "bins = np.linspace(-minmax, minmax, num=shades)\n",
        "palette_indices = np.digitize(selected_features, bins) - 1\n",
        "colors = [palette[idx] for idx in palette_indices]\n",
        "\n",
        "ax = sns.barplot(x=selected_features, y=selected_features.index, palette=colors, dodge=False)\n",
        "ax.set_yticklabels(selected_features.index)\n",
        "ax.set_xlabel(\"Feature impact on prediction\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ssw7FognQblZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Solution\n",
        "\n",
        "# 1. \"age\"\n",
        "# Since \"age\" is a continuous variable, the coefficient represents the difference in total compensation for each one-year difference in \"age\", if other variables remain the same.\n",
        "# It means that if \"age\" of employees differs by one year (the rest is constant) total compensation will increase by 32k USD, on average.\n",
        "\n",
        "# 2. \"job_level -> M8\" and \"generation -> Baby Boomers (1946-1964)\"\n",
        "# Since both \"job_level -> M8\" and \"generation -> Baby Boomers (1946-1964)\" are categorical variables, the coefficient is the average difference in total compensation between\n",
        "# the category for which, e.g. \"job_level -> M8\" = 0 (the reference group), i.e. employee is not at job level M8, and the category for which, e.g. \"job_level -> M8\" = 1 (the comparison group), i.e. employee is at job level M8.\n",
        "# It means that if employee is at \"job_level -> M8\" (and other variables remain constant) total compensation will increase by 57k USD, on average.\n",
        "# Next, it means that if employee was born as \"generation -> Baby Boomers (1946-1964)\" (and other variables remain constant) total compensation will decrease by 9k USD, on average."
      ],
      "metadata": {
        "cellView": "form",
        "id": "fadUPX6nEodc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4) Random Forest and Boruta feature selection coding example"
      ],
      "metadata": {
        "id": "L1e0rAoyS8FW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p2i1hBfFc18H"
      },
      "outputs": [],
      "source": [
        "#Split the data to train and test set, with test size being 20%\n",
        "X_data = worker_data.drop('total_compensation', axis=1)\n",
        "y_data = worker_data['total_compensation']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_data, y_data, test_size=0.20, random_state=12\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G8sYZrFDgdwq"
      },
      "outputs": [],
      "source": [
        "X_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TM7ZXC8Vgo5b"
      },
      "outputs": [],
      "source": [
        "X_test.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part A) Feature selection with Random Forest"
      ],
      "metadata": {
        "id": "RJyuBCy_Ggk4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 1: Fit RF model without feature selection"
      ],
      "metadata": {
        "id": "BT2xu8H-CJb0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fR0IJTtmgvNk"
      },
      "outputs": [],
      "source": [
        "# Model specification\n",
        "rf = RandomForestRegressor(n_estimators=150, max_depth=20, random_state=123)\n",
        "\n",
        "# Fitting the model on the train set\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Getting predictions on the test set\n",
        "y_pred = rf.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z5fSjqaUD0JV"
      },
      "outputs": [],
      "source": [
        "# Prediction metrics without feature selection\n",
        "print('Root Mean Squared Error (RMSE):', round(np.sqrt(metrics.mean_squared_error(y_test, y_pred)), 0))\n",
        "mape = np.mean(np.abs((y_test - y_pred) / np.abs(y_test)))\n",
        "print('Mean Absolute Percentage Error (MAPE):', round(mape * 100, 2), '%')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Step 2: Select the most important features using variable importance scores"
      ],
      "metadata": {
        "id": "AkEtIZ-MC7N7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify number of features to select\n",
        "MAX_FEATURES = 20"
      ],
      "metadata": {
        "id": "1SGEG797-eFA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SQ3Xc6f2kejC"
      },
      "outputs": [],
      "source": [
        "# Fit the selection \n",
        "sel = SelectFromModel(RandomForestRegressor(n_estimators=150, max_depth=20, random_state=123), \n",
        "                      threshold=-np.inf,\n",
        "                      max_features=MAX_FEATURES)\n",
        "sel.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q5fPnNKskvD2"
      },
      "outputs": [],
      "source": [
        "# Get the list of selected features - the ones with the highest feature importance\n",
        "selected_feat = X_train.columns[(sel.get_support())]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check how many and which features have been selected\n",
        "n_feat = len(selected_feat)\n",
        "print('Number of selected features: ', n_feat)"
      ],
      "metadata": {
        "id": "gf6-PUVVBBBi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BOGtDtQiDNF1"
      },
      "outputs": [],
      "source": [
        "# Visualise the features that have been selected and their respective scores\n",
        "feature_scores = pd.Series(rf.feature_importances_, index=X_train.columns).sort_values(ascending=False)\n",
        "\n",
        "f, ax = plt.subplots(figsize=(15, 5))\n",
        "ax = sns.barplot(x=feature_scores[:n_feat], y=feature_scores[:n_feat].index, data=df)\n",
        "ax.set_yticklabels(feature_scores[:n_feat].index, fontsize=7)\n",
        "ax.set_xlabel(\"Feature importance score\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: By visualising importances of more variables, we would just by visual comparison come to a similar onclusion about a cut-off - the last variable to choose would be maybe P2 job level or length of service."
      ],
      "metadata": {
        "id": "TZvNuQPTLhAX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Step 3: Fit the RF model again, now with selected features only"
      ],
      "metadata": {
        "id": "OoOGUJgtFegG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PdcnPn3IDM_q"
      },
      "outputs": [],
      "source": [
        "X_train_sel = X_train[selected_feat]\n",
        "X_test_sel = X_test[selected_feat]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6esuDR_tHTQv"
      },
      "outputs": [],
      "source": [
        "# Model specification\n",
        "rf_sel = RandomForestRegressor(n_estimators=150, max_depth=20, random_state=123)\n",
        "\n",
        "# Fitting the model on train data\n",
        "rf_sel.fit(X_train_sel, y_train)\n",
        "\n",
        "# Getting predictions on the test set\n",
        "y_pred_sel = rf_sel.predict(X_test_sel)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P4x1BvA3HgdA"
      },
      "outputs": [],
      "source": [
        "# Prediction metrics with feature selection\n",
        "print('Root Mean Squared Error (RMSE):', round(np.sqrt(metrics.mean_squared_error(y_test, y_pred_sel)), 0))\n",
        "mape = np.mean(np.abs((y_test - y_pred_sel) / np.abs(y_test)))\n",
        "print('Mean Absolute Percentage Error (MAPE):', round(mape * 100, 2), '%')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   At this point, we could use this model with selected features and fine-tune hyperparameters to obtain even better results"
      ],
      "metadata": {
        "id": "IbnayEbaGOnc"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hVa5BfBbS8L-"
      },
      "source": [
        "----"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part B) Feature selection with Boruta"
      ],
      "metadata": {
        "id": "Q3jiOy8_G5aZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Specify a base RF model and use it to run Boruta feature selection"
      ],
      "metadata": {
        "id": "-isU0Ld2X_rw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Base RF model specification\n",
        "rf = RandomForestRegressor(max_depth=20, random_state=123)"
      ],
      "metadata": {
        "id": "F4i4h_SpHpy_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Boruta feature search\n",
        "feat_selector = BorutaPy(\n",
        "    verbose=2,\n",
        "    estimator=rf,\n",
        "    n_estimators=20,\n",
        "    max_iter=25,\n",
        "    random_state=123\n",
        ")\n",
        "\n",
        "feat_selector.fit(np.array(X_train), np.array(y_train))"
      ],
      "metadata": {
        "id": "R56-yXyVHs-Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the list of selected features\n",
        "boruta_selected = []\n",
        "\n",
        "for i in range(len(feat_selector.support_)):\n",
        "    if (feat_selector.support_[i] or feat_selector.support_weak_[i]):\n",
        "        boruta_selected.append(X_train.columns[i])\n",
        "        print(X_train.columns[i])"
      ],
      "metadata": {
        "id": "hJMfkFjSMi9l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Number of selected features: ', len(boruta_selected))"
      ],
      "metadata": {
        "id": "ADoC6vN1i_si"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Exercise: Re-run Boruta selection with different parameters specification and use the selected features in random forest model.\n",
        "\n"
      ],
      "metadata": {
        "id": "RXsofMkkUe5V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### a) Run Boruta feature selection: use 50 trees in the forest and 20 iterations"
      ],
      "metadata": {
        "id": "buoZOXcbU1Ts"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Base RF model specification\n",
        "rf = RandomForestRegressor(max_depth=20, random_state=123)\n",
        "\n",
        "# Boruta feature search\n",
        "feat_selector = BorutaPy(\n",
        "    verbose=2,\n",
        "\n",
        "    # ??\n",
        "    \n",
        "    random_state=123\n",
        ")\n",
        "\n",
        "feat_selector.fit(np.array(X_train), np.array(y_train))"
      ],
      "metadata": {
        "id": "5lAmPSPTUypH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the list of selected features\n",
        "boruta_selected = []\n",
        "\n",
        "for i in range(len(feat_selector.support_)):\n",
        "    if (feat_selector.support_[i] or feat_selector.support_weak_[i]):\n",
        "        boruta_selected.append(X_train.columns[i])\n",
        "        print(X_train.columns[i])\n"
      ],
      "metadata": {
        "id": "eMlUBWz9UypI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Number of selected features: ', len(boruta_selected))"
      ],
      "metadata": {
        "id": "7-9yYfJM45f6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###b) Run RF model with features selected by Boruta"
      ],
      "metadata": {
        "id": "N4vDjhTdUypI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get subsets of explanatory variables only with features that were selected\n",
        "X_filtered_train = #??\n",
        "X_filtered_test = #??"
      ],
      "metadata": {
        "id": "v6Nu9uRSUypI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model specification \n",
        "rf = \n",
        "\n",
        "# Fit the model\n",
        "\n",
        "# Get predictions\n",
        "y_pred_BR_sel = "
      ],
      "metadata": {
        "id": "ldPQ7OkxUypJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prediction metrics with feature selection\n",
        "print('Root Mean Squared Error (RMSE):', np.sqrt(metrics.mean_squared_error(y_test, y_pred_BR_sel)))\n",
        "mape = np.mean(np.abs((y_test - y_pred_BR_sel) / np.abs(y_test)))\n",
        "print('Mean Absolute Percentage Error (MAPE):', round(mape * 100, 2))"
      ],
      "metadata": {
        "id": "YGc4tu_xUypJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Solution\n",
        "rf = RandomForestRegressor(max_depth=20, random_state=123)\n",
        "\n",
        "# Boruta feature search\n",
        "feat_selector = BorutaPy(\n",
        "    verbose=2,\n",
        "    estimator=rf,\n",
        "    n_estimators=50,\n",
        "    max_iter=20,\n",
        "    random_state=123\n",
        ")\n",
        "\n",
        "feat_selector.fit(np.array(X_train), np.array(y_train))\n",
        "\n",
        "# Get the list of selected features\n",
        "boruta_selected = []\n",
        "\n",
        "for i in range(len(feat_selector.support_)):\n",
        "    if (feat_selector.support_[i] or feat_selector.support_weak_[i]):\n",
        "        boruta_selected.append(X_train.columns[i])\n",
        "        print(X_train.columns[i])\n",
        "\n",
        "# Get subsets of explanatory variables only with features that were selected\n",
        "X_filtered_train = X_train[boruta_selected]\n",
        "X_filtered_test = X_test[boruta_selected]\n",
        "\n",
        "# Model specification\n",
        "rf = RandomForestRegressor(n_estimators=150, max_depth=20, random_state=123)\n",
        "\n",
        "# Fit the model\n",
        "rf.fit(X_filtered_train, y_train)\n",
        "\n",
        "# Get predictions\n",
        "y_pred_BR_sel = rf.predict(X_filtered_test)\n",
        "\n",
        "# Prediction metrics with feature selection\n",
        "print('Root Mean Squared Error (RMSE):', np.sqrt(metrics.mean_squared_error(y_test, y_pred_BR_sel)))\n",
        "mape = np.mean(np.abs((y_test - y_pred_BR_sel) / np.abs(y_test)))\n",
        "print('Mean Absolute Percentage Error (MAPE):', round(mape * 100, 2))"
      ],
      "metadata": {
        "id": "Wf_cP3xHJuOn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}